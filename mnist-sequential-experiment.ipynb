{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>96</td><td>application_1596125182098_0100</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://resourcemanager.service.consul:8088/proxy/application_1596125182098_0100/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopspom:8042/node/containerlogs/container_e01_1596125182098_0100_01_000001/mnist_experiment__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "def create_dataset():\n",
    "    from hops import hdfs\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python import keras\n",
    "    from tensorflow.python.keras import backend as K\n",
    "    \n",
    "    local_path = hdfs.copy_to_local('Resources/mnist.npz')\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path=local_path)\n",
    "    \n",
    "    batch_size = 512\n",
    "    num_classes = 10\n",
    "    \n",
    "    \n",
    "    # Input image dimensions\n",
    "    img_rows, img_cols = 28, 28\n",
    "    \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "        x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "        input_shape = (1, img_rows, img_cols)\n",
    "    else:\n",
    "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "        input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "\n",
    "    # Convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model_generator():\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python.keras.models import Sequential\n",
    "    from tensorflow.python.keras.layers import Dense, Dropout, Flatten\n",
    "    from tensorflow.python.keras.layers import Conv2D, MaxPooling2D\n",
    "    \n",
    "    kernel = 4\n",
    "    pool = 7\n",
    "    dropout = 0.85\n",
    "    num_classes = 10\n",
    "\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(kernel, kernel),\n",
    "                     activation='relu',\n",
    "                     input_shape=(28, 28, 1)))\n",
    "    model.add(Conv2D(64, (kernel, kernel), activation='relu', name='second_conv'))\n",
    "    model.add(MaxPooling2D(pool_size=(pool, pool)))\n",
    "    model.add(Dropout(dropout, name='first_dropout'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', name='dense_layer'))\n",
    "    model.add(Dropout(dropout, name='second_dropout'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Included single layers are: \n",
      "\n",
      "dense_layer\n",
      "first_dropout\n",
      "second_dropout\n",
      "second_conv"
     ]
    }
   ],
   "source": [
    "from maggy.ablation import AblationStudy\n",
    "\n",
    "ablation_study = AblationStudy(\"mnist\", 1, \"number\",)\n",
    "ablation_study.set_dataset_generator(create_dataset)\n",
    "ablation_study.model.set_base_model_generator(base_model_generator)\n",
    "\n",
    "ablation_study.model.layers.include('second_conv', 'first_dropout', 'dense_layer', 'second_dropout')\n",
    "\n",
    "ablation_study.model.layers.print_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_fn(dataset_function, model_function):\n",
    "    \n",
    "    from tensorflow.python import keras\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    #### enable GPU support for tf v1\n",
    "    tf.enable_eager_execution()\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "      # Restrict TensorFlow to only use the first GPU\n",
    "      try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "      except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "    #####\n",
    "\n",
    "    batch_size = 512\n",
    "    \n",
    "    x_train, y_train, x_test, y_test = dataset_function()\n",
    "    \n",
    "    model = model_function()\n",
    "\n",
    "    opt = keras.optimizers.Adadelta(1.0)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=10,\n",
    "              verbose=1,\n",
    "             )\n",
    "    score = model.evaluate(x_test, y_test, verbose=1)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    \n",
    "    return score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a2fd3ad90147d187a2ee336cc926f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Maggy experiment', max=5.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 1 Physical GPUs, 1 Logical GPU\n",
      "0: Started copying hdfs://rpc.namenode.service.consul:8020/Projects/mnist_experiment/Resources/mnist.npz to local disk on path /srv/hops/hopsdata/tmp/nm-local-dir/usercache/uFyNu0LHDiOecDnqtljDXsHQTvTCgSHB68VNua-PzPk/appcache/application_1596125182098_0100/container_e01_1596125182098_0100_01_000002/\n",
      "\n",
      "0: Finished copying\n",
      "\n",
      "0: x_train shape: (60000, 28, 28, 1)\n",
      "0: 60000 train samples\n",
      "0: 10000 test samples\n",
      "0: Train on 60000 samples\n",
      "0: Epoch 1/10\n",
      "0: Epoch 2/10\n",
      "0: Epoch 3/10\n",
      "0: Epoch 4/10\n",
      "0: Epoch 5/10\n",
      "0: Epoch 6/10\n",
      "0: Epoch 7/10\n",
      "0: Epoch 8/10\n",
      "0: Epoch 9/10\n",
      "0: Epoch 10/10\n",
      "0: Test loss: 0.5315898368835449\n",
      "0: Test accuracy: 0.9193\n",
      "0: 1 Physical GPUs, 1 Logical GPU\n",
      "0: File hdfs://rpc.namenode.service.consul:8020/Projects/mnist_experiment/Resources/mnist.npz is already localized, skipping download...\n",
      "0: x_train shape: (60000, 28, 28, 1)\n",
      "0: 60000 train samples\n",
      "0: 10000 test samples\n",
      "0: Train on 60000 samples\n",
      "0: Epoch 1/10\n",
      "0: Epoch 2/10\n",
      "0: Epoch 3/10\n",
      "0: Epoch 4/10\n",
      "0: Epoch 5/10\n",
      "0: Epoch 6/10\n",
      "0: Epoch 7/10\n",
      "0: Epoch 8/10\n",
      "0: Epoch 9/10\n",
      "0: Epoch 10/10\n",
      "0: Test loss: 0.05347622517067939\n",
      "0: Test accuracy: 0.9836\n",
      "0: 1 Physical GPUs, 1 Logical GPU\n",
      "0: File hdfs://rpc.namenode.service.consul:8020/Projects/mnist_experiment/Resources/mnist.npz is already localized, skipping download...\n",
      "0: x_train shape: (60000, 28, 28, 1)\n",
      "0: 60000 train samples\n",
      "0: 10000 test samples\n",
      "0: Train on 60000 samples\n",
      "0: Epoch 1/10\n",
      "0: Epoch 2/10\n",
      "0: Epoch 3/10\n",
      "0: Epoch 4/10\n",
      "0: Epoch 5/10\n",
      "0: Epoch 6/10\n",
      "0: Epoch 7/10\n",
      "0: Epoch 8/10\n",
      "0: Epoch 9/10\n",
      "0: Epoch 10/10\n",
      "0: Test loss: 0.038127421853896525\n",
      "0: Test accuracy: 0.9884\n",
      "0: 1 Physical GPUs, 1 Logical GPU\n",
      "0: File hdfs://rpc.namenode.service.consul:8020/Projects/mnist_experiment/Resources/mnist.npz is already localized, skipping download...\n",
      "0: x_train shape: (60000, 28, 28, 1)\n",
      "0: 60000 train samples\n",
      "0: 10000 test samples\n",
      "0: Train on 60000 samples\n",
      "0: Epoch 1/10\n",
      "0: Epoch 2/10\n",
      "0: Epoch 3/10\n",
      "0: Epoch 4/10\n",
      "0: Epoch 5/10\n",
      "0: Epoch 6/10\n",
      "0: Epoch 7/10\n",
      "0: Epoch 8/10\n",
      "0: Epoch 9/10\n",
      "0: Epoch 10/10\n",
      "0: Test loss: 0.3773805529594421\n",
      "0: Test accuracy: 0.9605\n",
      "0: 1 Physical GPUs, 1 Logical GPU\n",
      "0: File hdfs://rpc.namenode.service.consul:8020/Projects/mnist_experiment/Resources/mnist.npz is already localized, skipping download...\n",
      "0: x_train shape: (60000, 28, 28, 1)\n",
      "0: 60000 train samples\n",
      "0: 10000 test samples\n",
      "0: Train on 60000 samples\n",
      "0: Epoch 1/10\n",
      "0: Epoch 2/10\n",
      "0: Epoch 3/10\n",
      "0: Epoch 4/10\n",
      "0: Epoch 5/10\n",
      "0: Epoch 6/10\n",
      "0: Epoch 7/10\n",
      "0: Epoch 8/10\n",
      "0: Epoch 9/10\n",
      "0: Epoch 10/10\n",
      "0: Test loss: 0.12651749925017358\n",
      "0: Test accuracy: 0.9697\n",
      "Started Maggy Experiment: MNIST_LOCO_10_epochs, application_1596125182098_0100, run 1\n",
      "\n",
      "------ LOCO Results ------ \n",
      "BEST Config Excludes {\"ablated_feature\": \"None\", \"ablated_layer\": \"first_dropout\"} -- metric 0.9884\n",
      "WORST Config Excludes {\"ablated_feature\": \"None\", \"ablated_layer\": \"second_conv\"} -- metric 0.9193\n",
      "AVERAGE metric -- 0.964300000667572\n",
      "Total Job Time 0 hours, 2 minutes, 16 seconds\n",
      "\n",
      "Finished Experiment\n"
     ]
    }
   ],
   "source": [
    "from maggy import experiment\n",
    "\n",
    "result = experiment.lagom(map_fun=training_fn, experiment_type='ablation',\n",
    "                           ablation_study=ablation_study, \n",
    "                           ablator='loco', \n",
    "                           name='MNIST_LOCO_10_epochs'\n",
    "                          )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}