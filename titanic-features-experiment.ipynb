{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maggy Ablation: Feature Ablation for the Titanic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate Maggy's Feature Ablation API, while using a TensorFlow Keras Sequential model trained on the [Titanic Dataset](https://www.kaggle.com/c/titanic/data). To be able to follow along, make sure you have the Titanic training dataset registered on your Project's Feature Store, as explained [in this example notebook](https://github.com/logicalclocks/hops-examples/blob/master/notebooks/featurestore/datasets/TitanicTrainingDatasetPython.ipynb).\n",
    "\n",
    "## Wait ... What is an *Ablation Study*?\n",
    "\n",
    "An Ablation Study, in medical and psychological research, is a research method in which the roles and functions of an organ, tissue, or any part of a living organism, is examined through its surgical removal and observing the behaviour of the organism in its absence. This method, also known as experimental ablation, was pioneered by the French physiologist [Marie Jean Pierre Flourens](https://en.wikipedia.org/wiki/Jean_Pierre_Flourens) in the early nineteenth century. Flourens would perform ablative brain surgeries on animals, removing different parts of their nervous systems and observing the effects on their behaviour. This method has since been used in a variety of disciplines, but most prominently in medical and psychological research and neuroscience.\n",
    "\n",
    "## What Does it Have to Do with Machine Learning?\n",
    "\n",
    "In the context of machine learning, we can define ablation study as *“a scientific examination of a machine learning system by removing its building blocks in order to gain insight on their effects on its overall performance”*. Dataset features and model components are notable examples of these building blocks (hence we use their corresponding terms of **feature ablation** and **model ablation**), but any design choice or module of the system may be included in an ablation study.\n",
    "\n",
    "## Experiments and Trials\n",
    "\n",
    "We can think that an ablation study is an *experiment* that consists of several *trials*. For example, each model ablation trial involves training a model with one or more of its components (e.g. a layer) removed. Similarly, a feature ablation trial involves training a model using a different set of dataset features, and observing the outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Studies with Maggy\n",
    "\n",
    "With Maggy, performing ablation studies of your machine learning or deep learning systems is a fairly simple task that consists of the following steps:\n",
    "\n",
    "1. Creating an `AblationStudy` instance,\n",
    "2. Specifying the components that you want to ablate by *including* them in your `AblationStudy` instance,\n",
    "3. Defining a *base model generator function* and/or a *dataset generator function*,\n",
    "4. Wrapping your TensorFlow/Keras code in a Python function (let's call it **training function**) that receives two arguments (`model_function` and `dataset_function`), and\n",
    "5. Launching your experiment with Maggy while specifying an *ablation policy*.\n",
    "\n",
    "It's as simple as that.\n",
    "\n",
    "## What Changes Should I Make in my TensorFlow/Keras Code?\n",
    "\n",
    "Not so much. You'll see an example shortly, but the most important thing is:\n",
    "\n",
    "- For **model ablation**, you need to define a function that returns a TF/Keras `model`, and use that in your code instead of defining the model in your training function. If you want to perform **layer ablation**, then you should provide a `name` argument while adding layers to your `tf.keras.Sequential` model, and include those names in your `AblationStudy` instance as well.\n",
    "\n",
    "- For **feature ablation**:\n",
    "    - if you have your training dataset in the [**Feature Store**](https://www.logicalclocks.com/featurestorepage) in form of `tfrecord`, you can directly include the features you want to ablate using their names and calling a *dataset generator function* in your training function. The dataset generator functions will be created under the hood by maggy for each feature ablation trial.\n",
    "    - alternatively, you can define your own *dataset generator function* and pass it to your `AblationStudy` instance initializer as an argument. \n",
    "    \n",
    "Now let's see how this actually works.\n",
    "Get your `SparkSession` by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>91</td><td>application_1596125182098_0095</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://resourcemanager.service.consul:8088/proxy/application_1596125182098_0095/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopspom:8042/node/containerlogs/container_e01_1596125182098_0095_01_000001/titanic_experiment__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from hops import hdfs\n",
    "from hops import featurestore\n",
    "import maggy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create an `AblationStudy` instance. Here, the required arguments are 1) the name of your training dataset *as it is in your project's feature store*, and 2) * the name of the *label* column.\n",
    "\n",
    "You can also provide the version of your training dataset in the feature store, but the default version is `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create an AblationStudy instance.\n",
    "\n",
    "from maggy.ablation import AblationStudy\n",
    "\n",
    "ablation_study = AblationStudy('titanic_train_dataset', training_dataset_version=1,\n",
    "                              label_name='survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Ablation\n",
    "\n",
    "We perform feature ablation by **including** features in our `AblationStudy` instance. Including a feature means that there will be a trial where the model will be trained *without* that feature. In other words, you include features in the ablation study so that they will be excluded from the training dataset.\n",
    "\n",
    "We have the following features in our training dataset:\n",
    "\n",
    "`['age', 'fare', 'parch', 'pclass', 'sex', 'sibsp', 'survived']`\n",
    "\n",
    "You can include features using `features.include()` method of your `AblationStudy` instance, by passing the names of the features, either separately or as a list of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include features one by one\n",
    "\n",
    "ablation_study.features.include('pclass')\n",
    "\n",
    "# include a list of features\n",
    "\n",
    "list_of_features = ['fare', 'sibsp', 'sex', 'parch', 'age']\n",
    "ablation_study.features.include(list_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fare\n",
      "sibsp\n",
      "pclass\n",
      "age\n",
      "sex\n",
      "parch"
     ]
    }
   ],
   "source": [
    "ablation_study.features.list_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the base model generator function\n",
    "\n",
    "def base_model_generator():\n",
    "    import tensorflow as tf\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(64, name='my_dense_two', activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(2, name='my_dense_sigmoid', activation='sigmoid'))\n",
    "    # output layer\n",
    "    model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the base model generator\n",
    "\n",
    "ablation_study.model.set_base_model_generator(base_model_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to recap, the ablator will generate one trial per each feature included in the `AblationStudy` instance, and one base trial that contains all the features.\n",
    "\n",
    "Now the only thing you need to do is to wrap your training code in a Python function. You can name this function whatever you wish, but we will refer to it as the *training* or *wrapper* function. The `model_function` and `dataset_function` used in the code are generated by the ablator per each trial, and you should call them in your code. This is your everyday TensorFlow/Keras code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap your code in a Python function\n",
    "\n",
    "from maggy import experiment\n",
    "\n",
    "def training_fn(dataset_function, model_function):\n",
    "    import tensorflow as tf\n",
    "    epochs = 10\n",
    "    batch_size = 30\n",
    "    \n",
    "    # since no custom dataset function is provided, maggy will use its own\n",
    "    # dataset generator (implemented in ablation.utils package) to prepare\n",
    "    # the dataset from the project featurestore\n",
    "    dataset = dataset_function(epochs, batch_size)\n",
    "    \n",
    "    # 80% training, 20% test\n",
    "    split = 4\n",
    "    train_set = dataset.window(split, split + 1).flat_map(lambda *ds: ds[0] if len(ds) == 1 else tf.data.Dataset.zip(ds))\n",
    "    test_set = dataset.skip(split).window(1, split + 1).flat_map(lambda *ds: ds[0] if len(ds) == 1 else tf.data.Dataset.zip(ds))\n",
    "    \n",
    "    \n",
    "    model = model_function()\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(train_set, epochs=10, steps_per_epoch=30, verbose=0)\n",
    "    \n",
    "    test_score = model.evaluate(test_set)\n",
    "    \n",
    "    print('Test loss:', test_score[0])\n",
    "    print('Test accuracy:', test_score[1])\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    return test_score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0adc54075530487c9197359919061148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Maggy experiment', max=7.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Test loss: 0.5853615204493204\n",
      "0: Test accuracy: 0.68421054\n",
      "1: Test loss: 6.169979492823283\n",
      "1: Test accuracy: 0.5964912\n",
      "2: Test loss: 0.6550388385852178\n",
      "2: Test accuracy: 0.7134503\n",
      "0: Test loss: 6.169979492823283\n",
      "0: Test accuracy: 0.5964912\n",
      "3: Test loss: 6.169979492823283\n",
      "3: Test accuracy: 0.5964912\n",
      "1: Test loss: 0.5243698060512543\n",
      "1: Test accuracy: 0.7719298\n",
      "2: Test loss: 0.588441381851832\n",
      "2: Test accuracy: 0.6608187\n",
      "Started Maggy Experiment: TITANIC-LOCO-10-epochs-features, application_1596125182098_0095, run 1\n",
      "\n",
      "------ LOCO Results ------ \n",
      "BEST Config Excludes {\"ablated_feature\": \"fare\", \"ablated_layer\": \"None\"} -- metric 0.7719298\n",
      "WORST Config Excludes {\"ablated_feature\": \"sex\", \"ablated_layer\": \"None\"} -- metric 0.5964912\n",
      "AVERAGE metric -- 0.6599832858358111\n",
      "Total Job Time 0 hours, 0 minutes, 59 seconds\n",
      "\n",
      "Finished Experiment\n"
     ]
    }
   ],
   "source": [
    "# launch the experiment\n",
    "\n",
    "result = experiment.lagom(map_fun=training_fn, experiment_type='ablation',\n",
    "                           ablation_study=ablation_study, \n",
    "                           ablator='loco', \n",
    "                           name='TITANIC-LOCO-10-epochs-features'\n",
    "                          )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}